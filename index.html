<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xinqiang Yu | 于新强</title>
  
  <meta name="author" content="Xinqiang Yu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p class="name" style="text-align: center;">
                Xinqiang Yu | 于新强
              </p>
              Now, I am a jointly trained PhD candidate, supervised by <a href="https://hughw19.github.io"> Prof. He Wang </a> (Peking University, Galbot) and <a href="https://zhaoxiangzhang.net"> Prof. Zhaoxiang Zhang </a> (Institute of Automation, Chinese Academy of Sciences). And I am also honored to have received guidance from <a href="https://ericyi.github.io/"> Prof. Li Yi</a> (Tsinghua University, Galbot).
            <br><br>
            Before that, I obtained my B.S. in <a href="https://rjxy.lntu.edu.cn">Software Engineering from Liaoning Technical University</a>.
            And I got my master degree from <a href="http://www.ict.ac.cn">Institute of Computing Technology, Chinese Academy of Sciences</a>, supervised by <a href="https://winycg.github.io">Prof. Chuanguang Yang</a>, Prof. Zhulin An and Prof. Yongjun Xu.
            <br><br>
            My research focuses on Dexterous Manipulation(Embodied Intelligence), 3D Computer Vision and Multimodal Large Language Models.
            <br><br>
            
          </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.jpg"><img style="width:70%;max-width:70%" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Publications</h2>

              <p> *: equal contribution; †: corresponding author(s)</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
    <!-- fenge -->
    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/dexvlg.png' width="100%">
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
          <span class="papertitle">
          DexVLG: Dexterous Vision-Language-Grasp Model at Scale
          </span>
        <br>
        <a href="https://jiaweihe.com">Jiawei He</a>*,
        <a href="https://danshi-li.github.io/personal-website/">Danshi Li</a>*,
        <strong>Xinqiang Yu</strong>*,
        <a ref="https://qizekun.github.io/">Zekun Qi</a>,
        <a href="">Wenyao zhang</a>,
        <a href="https://jychen18.github.io">Jiayi Chen</a>, 
        <a href="https://zhaoxiangzhang.net">Zhaoxiang Zhang</a>,
        <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en">Zhizheng Zhang</a>,
        <a href="https://ericyi.github.io/">Li Yi</a>,
        <a href="https://hughw19.github.io/">He Wang†</a>
        <br>
        <em>Under review</em>
        <br>
        <p></p>
      </td>
    </tr>
    <!-- fenge -->

         <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/DexGraspNet2.png' width="100%">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
              <span class="papertitle">
                DexGraspNet 2.0: Learning Generative Dexterous Grasping in Large-scale Synthetic Cluttered Scenes
              </span>
            <br>
            <a href="https://mzhmxzh.github.io/">Jialiang Zhang*</a>,
            <a href="https://lhrrhl0419.github.io/">Haoran Liu*</a>,
            <a href="https://danshi-li.github.io/personal-website/">Danshi Li*</a>,
            <strong>Xinqiang Yu*</strong>,
            <a href="https://geng-haoran.github.io">Haoran Geng</a>,
            <a href="https://selina2023.github.io">Yufei Ding</a>,
            <a href="https://jychen18.github.io">Jiayi Chen</a>, 
            <a href="https://hughw19.github.io/">He Wang†</a>
            <br>
            <em>CoRL</em>, 2024
            <br>
            <a href="https://pku-epic.github.io/DexGraspNet2.0/">project page</a> /
            <a href="https://arxiv.org/abs/2410.23004">arXiv</a> /
            <a href="https://github.com/PKU-EPIC/DexGraspNet2">code</a>
            <p></p>
          </td>
       </tr>


      <!-- fenge -->
       <tr>
        <td style="padding:10px;width:30%;vertical-align:middle">
          <img src='images/MTKD_RL.png' width="100%">
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
            <span class="papertitle">
              Multi-Teacher Knowledge Distillation with Reinforcement Learning for Visual Recognition
            </span>
          <br>
          <a href="winycg.github.io">Chuanguang Yang</a>,
          <strong>Xinqiang Yu</strong>,
          <a href="">Han Yang</a>,
          <a href="">Zhulin An</a>,
          <a href="">Chengqing Yu</a>,
          <a href="">Libo Huang</a>,
          <a href="">Yongjun Xu</a>,

          <br>
          <em>AAAI</em>, 2025, <font color="red">Oral</font>
          <br>
          <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32990/35145">paper</a> /
          <a href="https://github.com/winycg/MTKD-RL">code</a>
          <p></p>
        </td>
     </tr>


     <!-- fenge -->
    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/OPD.jpg' width="100%">
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
          <span class="papertitle">
            Online Policy Distillation with Decision-Attention
          </span>
        <br>
        <strong>Xinqiang Yu</strong>,
        <a href="winycg.github.io">Chuanguang Yang</a>,
        <a href="">Chengqing Yu</a>,
        <a href="">Libo Huang</a>,
        <a href="">Zhulin An</a>,
        <a href="">Yongjun Xu</a>,

        <br>
        <em>IJCNN</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2406.05488">Paper</a>
        <p></p>
      </td>
    </tr>

      <!-- fenge -->
      <tr>
        <td style="padding:10px;width:30%;vertical-align:middle">
          <img src='images/sofar.png' width="100%">
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
            <span class="papertitle">
              SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation
            </span>
          <br>
          <a ref="https://qizekun.github.io/">Zekun Qi</a> *,
          <a href="">Wenyao Zhang</a> *, 
          <a href="https://selina2023.github.io/">Yufei Ding</a> *, 
          <a href="https://runpeidong.web.illinois.edu/">Runpei Dong</a>,
          <strong>Xinqiang Yu</strong>,
          <a href="">Jingwen Li</a>,
          <a hferf="">Lingyun Xu</a>,
          <a href="https://baoyuli.github.io/">Baoyu Li</a>,
          <a href="https://xialin-he.github.io/">Xialin He</a>,
          <a href="https://github.com/Asterisci/">Guofan Fan</a>,
          <a href="https://jzhzhang.github.io/">Jiazhao Zhang</a>,
          <a href="https://jiaweihe.com">Jiawei He</a>,
          <a href="https://hughw19.github.io/">Jiayuan Gu</a>
          <a href="http://home.ustc.edu.cn/~jinxustc/">Xin Jin</a>
          <a href="http://group.iiis.tsinghua.edu.cn/~maks/leader.html">Kaisheng Ma</a>
          <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en">Zhizheng Zhang</a>
          <a href="https://hughw19.github.io/">He Wang</a>
          <a href="https://ericyi.github.io/">Li Yi</a>
          <br>
          <em>Under review</em>
          <br>
          <a href="https://qizekun.github.io/sofar/">project page</a> /
          <a href="https://arxiv.org/abs/2502.13143">arXiv</a> /
          <a href="https://github.com/qizekun/SoFar">code</a> /
          <a href="https://huggingface.co/collections/qizekun/sofar-67b511129d3146d28cea9920">huggingface</a>
          <p></p>
        </td>
     </tr>

     <!-- fenge -->
     <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/omnispatial.jpg' width="100%">
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
          <span class="papertitle">
            OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models
          </span>
        <br>
        <a href="">Mengdi Jia</a>*,
        <a href="https://qizekun.github.io">Zekun Qi</a>*,
        <a href="">Shaochen Zhang</a>,
        <a href="">Wenyao Zhang</a>,
        <strong>Xinqiang Yu</strong>,
        <a href="https://jiaweihe.com">Jiawei He</a>,
        <a href="https://hughw19.github.io/">He Wang</a>,
        <a href="https://ericyi.github.io/">Li Yi</a>
        <br>
        <em>Under review</em>
        <br>
        <a href="https://qizekun.github.io/omnispatial/">project page</a> /
        <a href="https://arxiv.org/abs/2506.03135">arXiv</a> /
        <a href="https://github.com/qizekun/OmniSpatial">code</a> /
        <a href="https://huggingface.co/datasets/qizekun/OmniSpatial">huggingface</a>
        <p></p>
      </td>
    </tr>

    <!-- fenge -->
    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/Springer_2023_survey.jpg' width="100%">
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
          <span class="papertitle">
            Categories of Response-Based, Feature-Based, and Relation-Based Knowledge Distillation
          </span>
        <br>
        <a href="https://winycg.github.io">Chuanguang Yang</a>,
        <strong>Xinqiang Yu</strong>,
        <a href="">Zhulin An</a>,
        <a href="">Yongjun Xu</a>
        <br>
        <em>Advancements in Knowledge Distillation: Towards New Horizons of Intelligent Systems</i> (Springer Book Chapter)</em>, <font color="red">Invited Survey Paper</font>
        <br>
        <a href="https://arxiv.org/pdf/2306.10687.pdf">arXiv</a>
        <p></p>
      </td>
    </tr>
    <!-- fenge -->
    
    <!-- fenge -->
    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/CLIP_KD.jpg' width="100%">
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
          <span class="papertitle">
            CLIP-KD: An Empirical Study of CLIP Model Distillation
          </span>
        <br>
        <a href="https://winycg.github.io">Chuanguang Yang</a>,
        <a href="">Zhulin An</a>,
        <a href="">Libo Huang</a>,
        <a href="">Junyu Bi</a>,
        <strong>Xinqiang Yu</strong>,
        <a href="">Han Yang</a>,
        <a href="">Boyu Diao</a>,
        <a href="">Yongjun Xu</a>
        <br>
        <em>CVPR</em>, 2024
        <br>
        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_CLIP-KD_An_Empirical_Study_of_CLIP_Model_Distillation_CVPR_2024_paper.pdf">paper</a>,
        <a href="https://github.com/winycg/CLIP-KD">Code</a>
        <p></p>
      </td>
    </tr>
    <!-- fenge -->
    
    <!-- fenge -->
    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/IL.png' width="100%">
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
          <span class="papertitle">
            Exemplar-Free Class Incremental Learning via Incremental Representation
          </span>
        <br>
        <a href="">Libo Huang</a>,
        <a href="">Zhulin An</a>,
        <a href="">Yan Zeng</a>,
        <a href="">Chuanguang Yang</a>,
        <strong>Xinqiang Yu</strong>,
        <a href="">Yongjun Xu</a>
        <br>
        <br>
        <a href="https://arxiv.org/pdf/2403.16221">paper</a>
        <p></p>
      </td>
    </tr>
    <!-- fenge -->


        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h2>Honors and Awards</h2>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="10"><tbody>
          <tr>
            <td width="80%" valign="center">
              <b>China National Scholarship</b> / <font color="red">  0.2%</font>
            </td>
          </tr>
          <tr>
            <td width="80%" valign="center">
              <b>First-Class College Scholarship</b> / <front>University of Chinese Academy of Sciences</front>
            </td>
          </tr>

        </tbody></table>


		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              Template stolen from <a href="https://jonbarron.info/">Jon Barron</a>.
              <br> Last updated: June 14, 2025
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>